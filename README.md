### 机器学习

[\[1112.6209\] Building high-level features using large scale unsupervised learning](https://arxiv.org/abs/1112.6209)

[\[1207.0580\] Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580)

[\[1302.4389\] Maxout Networks](https://arxiv.org/abs/1302.4389)

[\[1312.5602\] Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)

[\[1312.6114\] Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)

[\[1401.4082\] Stochastic Backpropagation and Approximate Inference in Deep Generative Models](https://arxiv.org/abs/1401.4082)

[\[1403.6652\] DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652)

[\[1404.7828\] Deep Learning in Neural Networks: An Overview](https://arxiv.org/abs/1404.7828)

[\[1406.2661\] Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)

[\[1406.5298\] Semi-Supervised Learning with Deep Generative Models](https://arxiv.org/abs/1406.5298)

[\[1406.6247\] Recurrent Models of Visual Attention](https://arxiv.org/abs/1406.6247)

[\[1411.1784\] Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)

[\[1411.1792\] How transferable are features in deep neural networks?](https://arxiv.org/abs/1411.1792)

[\[1412.3555\] Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555)

[\[1412.6572\] Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)

[\[1412.6980\] Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)

[\[1502.03167\] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)

[\[1502.05477\] Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)

[\[1503.02531\] Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)

[\[1503.03578\] LINE: Large-scale Information Network Embedding](https://arxiv.org/abs/1503.03578)

[\[1504.00702\] End-to-End Training of Deep Visuomotor Policies](https://arxiv.org/abs/1504.00702)

[\[1505.05424\] Weight Uncertainty in Neural Networks](https://arxiv.org/abs/1505.05424)

[\[1505.05770\] Variational Inference with Normalizing Flows](https://arxiv.org/abs/1505.05770)

[\[1505.07818\] Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818)

[\[1506.02142\] Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/abs/1506.02142)

[\[1506.02438\] High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)

[\[1507.06228\] Training Very Deep Networks](https://arxiv.org/abs/1507.06228)

[\[1509.02971\] Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971)

[\[1509.06461\] Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)

[\[1511.02274\] Stacked Attention Networks for Image Question Answering](https://arxiv.org/abs/1511.02274)

[\[1511.05493\] Gated Graph Sequence Neural Networks](https://arxiv.org/abs/1511.05493)

[\[1511.05952\] Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)

[\[1511.06434\] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)

[\[1511.06581\] Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)

[\[1602.01783\] Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)

[\[1602.02830\] Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://arxiv.org/abs/1602.02830)

[\[1602.04938\] "Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938)

[\[1602.07868\] Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks](https://arxiv.org/abs/1602.07868)

[\[1603.00748\] Continuous Deep Q-Learning with Model-based Acceleration](https://arxiv.org/abs/1603.00748)

[\[1603.02199\] Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection](https://arxiv.org/abs/1603.02199)

[\[1605.06676\] Learning to Communicate with Deep Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1605.06676)

[\[1606.01868\] Unifying Count-Based Exploration and Intrinsic Motivation](https://arxiv.org/abs/1606.01868)

[\[1606.02647\] Safe and Efficient Off-Policy Reinforcement Learning](https://arxiv.org/abs/1606.02647)

[\[1606.03498\] Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)

[\[1606.03657\] InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657)

[\[1606.04080\] Matching Networks for One Shot Learning](https://arxiv.org/abs/1606.04080)

[\[1606.04474\] Learning to learn by gradient descent by gradient descent](https://arxiv.org/abs/1606.04474)

[\[1606.04671\] Progressive Neural Networks](https://arxiv.org/abs/1606.04671)

[\[1606.05908\] Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908)

[\[1606.07792\] Wide & Deep Learning for Recommender Systems](https://arxiv.org/abs/1606.07792)

[\[1606.09375\] Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://arxiv.org/abs/1606.09375)

[\[1607.00653\] node2vec: Scalable Feature Learning for Networks](https://arxiv.org/abs/1607.00653)

[\[1607.06450\] Layer Normalization](https://arxiv.org/abs/1607.06450)

[\[1607.08022\] Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022)

[\[1608.03983\] SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983)

[\[1609.02907\] Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)

[\[1610.00633\] Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates](https://arxiv.org/abs/1610.00633)

[\[1610.09585\] Conditional Image Synthesis With Auxiliary Classifier GANs](https://arxiv.org/abs/1610.09585)

[\[1611.00712\] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables](https://arxiv.org/abs/1611.00712)

[\[1611.01578\] Neural Architecture Search with Reinforcement Learning](https://arxiv.org/abs/1611.01578)

[\[1611.02167\] Designing Neural Network Architectures using Reinforcement Learning](https://arxiv.org/abs/1611.02167)

[\[1611.03530\] Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)

[\[1611.05397\] Reinforcement Learning with Unsupervised Auxiliary Tasks](https://arxiv.org/abs/1611.05397)

[\[1611.07308\] Variational Graph Auto-Encoders](https://arxiv.org/abs/1611.07308)

[\[1612.00796\] Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/1612.00796)

[\[1612.01474\] Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles](https://arxiv.org/abs/1612.01474)

[\[1701.06538\] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)

[\[1701.06548\] Regularizing Neural Networks by Penalizing Confident Output Distributions](https://arxiv.org/abs/1701.06548)

[\[1701.07875\] Wasserstein GAN](https://arxiv.org/abs/1701.07875)

[\[1703.01780\] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results](https://arxiv.org/abs/1703.01780)

[\[1703.03400\] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)

[\[1703.05175\] Prototypical Networks for Few-shot Learning](https://arxiv.org/abs/1703.05175)

[\[1703.06103\] Modeling Relational Data with Graph Convolutional Networks](https://arxiv.org/abs/1703.06103)

[\[1704.01212\] Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212)

[\[1704.03732\] Deep Q-learning from Demonstrations](https://arxiv.org/abs/1704.03732)

[\[1705.07204\] Ensemble Adversarial Training: Attacks and Defenses](https://arxiv.org/abs/1705.07204)

[\[1705.07874\] A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)

[\[1706.02216\] Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216)

[\[1706.02275\] Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275)

[\[1706.06083\] Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083)

[\[1706.08500\] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500)

[\[1706.10295\] Noisy Networks for Exploration](https://arxiv.org/abs/1706.10295)

[\[1707.01926\] Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting](https://arxiv.org/abs/1707.01926)

[\[1707.02286\] Emergence of Locomotion Behaviours in Rich Environments](https://arxiv.org/abs/1707.02286)

[\[1707.03141\] A Simple Neural Attentive Meta-Learner](https://arxiv.org/abs/1707.03141)

[\[1707.06347\] Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)

[\[1707.06887\] A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887)

[\[1710.02298\] Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)

[\[1710.10196\] Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196)

[\[1710.10903\] Graph Attention Networks](https://arxiv.org/abs/1710.10903)

[\[1711.00937\] Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)

[\[1711.03938\] CARLA: An Open Urban Driving Simulator](https://arxiv.org/abs/1711.03938)

[\[1711.04043\] Few-Shot Learning with Graph Neural Networks](https://arxiv.org/abs/1711.04043)

[\[1711.10907\] Deep Reinforcement Learning for De-Novo Drug Design](https://arxiv.org/abs/1711.10907)

[\[1712.01815\] Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815)

[\[1801.01290\] Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290)

[\[1801.04406\] Which Training Methods for GANs do actually Converge?](https://arxiv.org/abs/1801.04406)

[\[1801.10247\] FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling](https://arxiv.org/abs/1801.10247)

[\[1802.00420\] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/abs/1802.00420)

[\[1802.01548\] Regularized Evolution for Image Classifier Architecture Search](https://arxiv.org/abs/1802.01548)

[\[1802.01561\] IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures](https://arxiv.org/abs/1802.01561)

[\[1802.03268\] Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/abs/1802.03268)

[\[1802.05957\] Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957)

[\[1802.09477\] Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477)

[\[1803.08494\] Group Normalization](https://arxiv.org/abs/1803.08494)

[\[1805.07722\] Task-Agnostic Meta-Learning for Few-shot Learning](https://arxiv.org/abs/1805.07722)

[\[1805.08318\] Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318)

[\[1805.11604\] How Does Batch Normalization Help Optimization?](https://arxiv.org/abs/1805.11604)

[\[1806.01261\] Relational inductive biases, deep learning, and graph networks](https://arxiv.org/abs/1806.01261)

[\[1806.01973\] Graph Convolutional Neural Networks for Web-Scale Recommender Systems](https://arxiv.org/abs/1806.01973)

[\[1806.02473\] Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation](https://arxiv.org/abs/1806.02473)

[\[1806.07366\] Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366)

[\[1806.09055\] DARTS: Differentiable Architecture Search](https://arxiv.org/abs/1806.09055)

[\[1807.00734\] The relativistic discriminator: a key element missing from standard GAN](https://arxiv.org/abs/1807.00734)

[\[1807.03039\] Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/abs/1807.03039)

[\[1807.03748\] Representation Learning with Contrastive Predictive Coding](https://arxiv.org/abs/1807.03748)

[\[1807.05960\] Meta-Learning with Latent Embedding Optimization](https://arxiv.org/abs/1807.05960)

[\[1808.06670\] Learning deep representations by mutual information estimation and maximization](https://arxiv.org/abs/1808.06670)

[\[1809.01999\] Recurrent World Models Facilitate Policy Evolution](https://arxiv.org/abs/1809.01999)

[\[1809.11096\] Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096)

[\[1810.07218\] Incremental Few-Shot Learning with Attention Attractor Networks](https://arxiv.org/abs/1810.07218)

[\[1810.09502\] How to train your MAML](https://arxiv.org/abs/1810.09502)

[\[1811.03962\] A Convergence Theory for Deep Learning via Over-Parameterization](https://arxiv.org/abs/1811.03962)

[\[1812.00332\] ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware](https://arxiv.org/abs/1812.00332)

[\[1812.04948\] A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948)

[\[1812.09926\] SNAS: Stochastic Neural Architecture Search](https://arxiv.org/abs/1812.09926)

[\[1902.06720\] Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent](https://arxiv.org/abs/1902.06720)

[\[1902.10197\] RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space](https://arxiv.org/abs/1902.10197)

[\[1903.00374\] Model-Based Reinforcement Learning for Atari](https://arxiv.org/abs/1903.00374)

[\[1903.07293\] Heterogeneous Graph Attention Network](https://arxiv.org/abs/1903.07293)

[\[1904.08082\] Self-Attention Graph Pooling](https://arxiv.org/abs/1904.08082)

[\[1904.09237\] On the Convergence of Adam and Beyond](https://arxiv.org/abs/1904.09237)

[\[1904.12848\] Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848)

[\[1905.00414\] Similarity of Neural Network Representations Revisited](https://arxiv.org/abs/1905.00414)

[\[1905.05301\] Hierarchically Structured Meta-learning](https://arxiv.org/abs/1905.05301)

[\[1905.06549\] TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning](https://arxiv.org/abs/1905.06549)

[\[1905.07953\] Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks](https://arxiv.org/abs/1905.07953)

[\[1905.12265\] Strategies for Pre-training Graph Neural Networks](https://arxiv.org/abs/1905.12265)

[\[1906.00446\] Generating Diverse High-Fidelity Images with VQ-VAE-2](https://arxiv.org/abs/1906.00446)

[\[1906.00910\] Learning Representations by Maximizing Mutual Information Across Views](https://arxiv.org/abs/1906.00910)

[\[1906.02530\] Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift](https://arxiv.org/abs/1906.02530)

[\[1906.02629\] When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629)

[\[1907.04931\] GraphSAINT: Graph Sampling Based Inductive Learning Method](https://arxiv.org/abs/1907.04931)

[\[1907.08610\] Lookahead Optimizer: k steps forward, 1 step back](https://arxiv.org/abs/1907.08610)

[\[1908.01000\] InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization](https://arxiv.org/abs/1908.01000)

[\[1908.03265\] On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265)

[\[1909.00025\] Meta-Learning with Warped Gradient Descent](https://arxiv.org/abs/1909.00025)

[\[1909.09157\] Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML](https://arxiv.org/abs/1909.09157)

[\[1911.06455\] Graph Transformer Networks](https://arxiv.org/abs/1911.06455)

[\[1911.08265\] Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/abs/1911.08265)

[\[1912.01603\] Dream to Control: Learning Behaviors by Latent Imagination](https://arxiv.org/abs/1912.01603)

[\[1912.01703\] PyTorch: An Imperative Style, High-Performance Deep Learning Library](https://arxiv.org/abs/1912.01703)

[\[1912.02762\] Normalizing Flows for Probabilistic Modeling and Inference](https://arxiv.org/abs/1912.02762)

[\[1912.02781\] AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty](https://arxiv.org/abs/1912.02781)

[\[1912.03820\] Meta-Learning without Memorization](https://arxiv.org/abs/1912.03820)

[\[2002.01680\] MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding](https://arxiv.org/abs/2002.01680)

[\[2002.09405\] Learning to Simulate Complex Physics with Graph Networks](https://arxiv.org/abs/2002.09405)

[\[2003.01332\] Heterogeneous Graph Transformer](https://arxiv.org/abs/2003.01332)

[\[2003.10580\] Meta Pseudo Labels](https://arxiv.org/abs/2003.10580)

[\[2006.05582\] Contrastive Multi-View Representation Learning on Graphs](https://arxiv.org/abs/2006.05582)

[\[2006.07733\] Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/abs/2006.07733)

[\[2006.09661\] Implicit Neural Representations with Periodic Activation Functions](https://arxiv.org/abs/2006.09661)

[\[2006.09963\] GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training](https://arxiv.org/abs/2006.09963)

[\[2006.10029\] Big Self-Supervised Models are Strong Semi-Supervised Learners](https://arxiv.org/abs/2006.10029)

[\[2111.09266\] GFlowNet Foundations](https://arxiv.org/abs/2111.09266)

### 计算机视觉

[\[1310.1531\] DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition](https://arxiv.org/abs/1310.1531)

[\[1311.2524\] Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)

[\[1311.2901\] Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)

[\[1312.4400\] Network In Network](https://arxiv.org/abs/1312.4400)

[\[1312.6034\] Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034)

[\[1312.6199\] Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199)

[\[1312.6229\] OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks](https://arxiv.org/abs/1312.6229)

[\[1403.6382\] CNN Features off-the-shelf: an Astounding Baseline for Recognition](https://arxiv.org/abs/1403.6382)

[\[1404.7584\] High-Speed Tracking with Kernelized Correlation Filters](https://arxiv.org/abs/1404.7584)

[\[1405.0312\] Microsoft COCO: Common Objects in Context](https://arxiv.org/abs/1405.0312)

[\[1405.3531\] Return of the Devil in the Details: Delving Deep into Convolutional Nets](https://arxiv.org/abs/1405.3531)

[\[1406.2199\] Two-Stream Convolutional Networks for Action Recognition in Videos](https://arxiv.org/abs/1406.2199)

[\[1406.4729\] Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729)

[\[1409.0575\] ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575)

[\[1409.1556\] Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)

[\[1409.4842\] Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)

[\[1411.4038\] Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038)

[\[1411.4389\] Long-term Recurrent Convolutional Networks for Visual Recognition and Description](https://arxiv.org/abs/1411.4389)

[\[1411.4555\] Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555)

[\[1412.0767\] Learning Spatiotemporal Features with 3D Convolutional Networks](https://arxiv.org/abs/1412.0767)

[\[1412.1897\] Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images](https://arxiv.org/abs/1412.1897)

[\[1412.2306\] Deep Visual-Semantic Alignments for Generating Image Descriptions](https://arxiv.org/abs/1412.2306)

[\[1412.7062\] Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs](https://arxiv.org/abs/1412.7062)

[\[1501.00092\] Image Super-Resolution Using Deep Convolutional Networks](https://arxiv.org/abs/1501.00092)

[\[1502.01852\] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)

[\[1502.03044\] Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)

[\[1502.03240\] Conditional Random Fields as Recurrent Neural Networks](https://arxiv.org/abs/1502.03240)

[\[1502.04623\] DRAW: A Recurrent Neural Network For Image Generation](https://arxiv.org/abs/1502.04623)

[\[1503.03832\] FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/abs/1503.03832)

[\[1504.08083\] Fast R-CNN](https://arxiv.org/abs/1504.08083)

[\[1505.04597\] U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)

[\[1506.01497\] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)

[\[1506.02025\] Spatial Transformer Networks](https://arxiv.org/abs/1506.02025)

[\[1506.02640\] You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)

[\[1506.05751\] Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks](https://arxiv.org/abs/1506.05751)

[\[1508.06576\] A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)

[\[1510.00149\] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149)

[\[1511.00561\] SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation](https://arxiv.org/abs/1511.00561)

[\[1511.04587\] Accurate Image Super-Resolution Using Very Deep Convolutional Networks](https://arxiv.org/abs/1511.04587)

[\[1511.05298\] Structural-RNN: Deep Learning on Spatio-Temporal Graphs](https://arxiv.org/abs/1511.05298)

[\[1511.07122\] Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122)

[\[1512.00567\] Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)

[\[1512.02325\] SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)

[\[1512.03385\] Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)

[\[1512.04150\] Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150)

[\[1601.06759\] Pixel Recurrent Neural Networks](https://arxiv.org/abs/1601.06759)

[\[1602.01528\] EIE: Efficient Inference Engine on Compressed Deep Neural Network](https://arxiv.org/abs/1602.01528)

[\[1602.07261\] Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)

[\[1602.07360\] SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size](https://arxiv.org/abs/1602.07360)

[\[1603.05027\] Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)

[\[1603.05279\] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/abs/1603.05279)

[\[1603.06937\] Stacked Hourglass Networks for Human Pose Estimation](https://arxiv.org/abs/1603.06937)

[\[1603.08155\] Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)

[\[1603.08511\] Colorful Image Colorization](https://arxiv.org/abs/1603.08511)

[\[1604.01685\] The Cityscapes Dataset for Semantic Urban Scene Understanding](https://arxiv.org/abs/1604.01685)

[\[1604.03540\] Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/abs/1604.03540)

[\[1604.06573\] Convolutional Two-Stream Network Fusion for Video Action Recognition](https://arxiv.org/abs/1604.06573)

[\[1604.07379\] Context Encoders: Feature Learning by Inpainting](https://arxiv.org/abs/1604.07379)

[\[1605.05396\] Generative Adversarial Text to Image Synthesis](https://arxiv.org/abs/1605.05396)

[\[1605.06211\] Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1605.06211)

[\[1605.06409\] R-FCN: Object Detection via Region-based Fully Convolutional Networks](https://arxiv.org/abs/1605.06409)

[\[1605.07146\] Wide Residual Networks](https://arxiv.org/abs/1605.07146)

[\[1606.00915\] DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https://arxiv.org/abs/1606.00915)

[\[1606.05328\] Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/abs/1606.05328)

[\[1606.07536\] Coupled Generative Adversarial Networks](https://arxiv.org/abs/1606.07536)

[\[1606.09549\] Fully-Convolutional Siamese Networks for Object Tracking](https://arxiv.org/abs/1606.09549)

[\[1607.02533\] Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533)

[\[1608.00859\] Temporal Segment Networks: Towards Good Practices for Deep Action Recognition](https://arxiv.org/abs/1608.00859)

[\[1608.03773\] Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking](https://arxiv.org/abs/1608.03773)

[\[1608.06993\] Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)

[\[1608.08710\] Pruning Filters for Efficient ConvNets](https://arxiv.org/abs/1608.08710)

[\[1609.03552\] Generative Visual Manipulation on the Natural Image Manifold](https://arxiv.org/abs/1609.03552)

[\[1609.04802\] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802)

[\[1609.05143\] Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning](https://arxiv.org/abs/1609.05143)

[\[1609.05158\] Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158)

[\[1610.02357\] Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)

[\[1610.02391\] Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391)

[\[1610.07584\] Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling](https://arxiv.org/abs/1610.07584)

[\[1611.04076\] Least Squares Generative Adversarial Networks](https://arxiv.org/abs/1611.04076)

[\[1611.05431\] Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431)

[\[1611.07004\] Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)

[\[1611.08050\] Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1611.08050)

[\[1611.09326\] The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/abs/1611.09326)

[\[1611.10012\] Speed/accuracy trade-offs for modern convolutional object detectors](https://arxiv.org/abs/1611.10012)

[\[1612.00593\] PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593)

[\[1612.01105\] Pyramid Scene Parsing Network](https://arxiv.org/abs/1612.01105)

[\[1612.03144\] Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144)

[\[1612.03242\] StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/abs/1612.03242)

[\[1612.08242\] YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242)

[\[1702.05464\] Adversarial Discriminative Domain Adaptation](https://arxiv.org/abs/1702.05464)

[\[1703.06211\] Deformable Convolutional Networks](https://arxiv.org/abs/1703.06211)

[\[1703.06868\] Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization](https://arxiv.org/abs/1703.06868)

[\[1703.06870\] Mask R-CNN](https://arxiv.org/abs/1703.06870)

[\[1703.10593\] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)

[\[1704.00028\] Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028)

[\[1704.02901\] Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs](https://arxiv.org/abs/1704.02901)

[\[1704.04861\] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)

[\[1704.06904\] Residual Attention Network for Image Classification](https://arxiv.org/abs/1704.06904)

[\[1705.07750\] Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset](https://arxiv.org/abs/1705.07750)

[\[1706.02413\] PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](https://arxiv.org/abs/1706.02413)

[\[1706.05587\] Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)

[\[1707.01083\] ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://arxiv.org/abs/1707.01083)

[\[1707.02921\] Enhanced Deep Residual Networks for Single Image Super-Resolution](https://arxiv.org/abs/1707.02921)

[\[1707.07012\] Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/abs/1707.07012)

[\[1707.07998\] Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](https://arxiv.org/abs/1707.07998)

[\[1708.02002\] Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)

[\[1709.01507\] Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)

[\[1710.09829\] Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829)

[\[1711.03213\] CyCADA: Cycle-Consistent Adversarial Domain Adaptation](https://arxiv.org/abs/1711.03213)

[\[1711.04340\] Data Augmentation Generative Adversarial Networks](https://arxiv.org/abs/1711.04340)

[\[1711.06025\] Learning to Compare: Relation Network for Few-Shot Learning](https://arxiv.org/abs/1711.06025)

[\[1711.06897\] Single-Shot Refinement Neural Network for Object Detection](https://arxiv.org/abs/1711.06897)

[\[1711.07767\] Receptive Field Block Net for Accurate and Fast Object Detection](https://arxiv.org/abs/1711.07767)

[\[1711.07971\] Non-local Neural Networks](https://arxiv.org/abs/1711.07971)

[\[1711.09020\] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](https://arxiv.org/abs/1711.09020)

[\[1711.10485\] AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks](https://arxiv.org/abs/1711.10485)

[\[1711.10925\] Deep Image Prior](https://arxiv.org/abs/1711.10925)

[\[1711.11248\] A Closer Look at Spatiotemporal Convolutions for Action Recognition](https://arxiv.org/abs/1711.11248)

[\[1711.11585\] High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](https://arxiv.org/abs/1711.11585)

[\[1712.00559\] Progressive Neural Architecture Search](https://arxiv.org/abs/1712.00559)

[\[1712.00726\] Cascade R-CNN: Delving into High Quality Object Detection](https://arxiv.org/abs/1712.00726)

[\[1712.04621\] The Effectiveness of Data Augmentation in Image Classification using Deep Learning](https://arxiv.org/abs/1712.04621)

[\[1801.03924\] The Unreasonable Effectiveness of Deep Features as a Perceptual Metric](https://arxiv.org/abs/1801.03924)

[\[1801.04381\] MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)

[\[1801.07455\] Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition](https://arxiv.org/abs/1801.07455)

[\[1801.07698\] ArcFace: Additive Angular Margin Loss for Deep Face Recognition](https://arxiv.org/abs/1801.07698)

[\[1801.07829\] Dynamic Graph CNN for Learning on Point Clouds](https://arxiv.org/abs/1801.07829)

[\[1802.02611\] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1802.02611)

[\[1802.08797\] Residual Dense Network for Image Super-Resolution](https://arxiv.org/abs/1802.08797)

[\[1803.01229\] GAN-based Synthetic Medical Image Augmentation for increased CNN Performance in Liver Lesion Classification](https://arxiv.org/abs/1803.01229)

[\[1803.07728\] Unsupervised Representation Learning by Predicting Image Rotations](https://arxiv.org/abs/1803.07728)

[\[1804.02767\] YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767)

[\[1804.04732\] Multimodal Unsupervised Image-to-Image Translation](https://arxiv.org/abs/1804.04732)

[\[1804.08328\] Taskonomy: Disentangling Task Transfer Learning](https://arxiv.org/abs/1804.08328)

[\[1804.09458\] Dynamic Few-Shot Visual Learning without Forgetting](https://arxiv.org/abs/1804.09458)

[\[1805.09501\] AutoAugment: Learning Augmentation Policies from Data](https://arxiv.org/abs/1805.09501)

[\[1805.11724\] Rethinking Knowledge Graph Propagation for Zero-Shot Learning](https://arxiv.org/abs/1805.11724)

[\[1807.05520\] Deep Clustering for Unsupervised Learning of Visual Features](https://arxiv.org/abs/1807.05520)

[\[1807.06521\] CBAM: Convolutional Block Attention Module](https://arxiv.org/abs/1807.06521)

[\[1807.11164\] ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design](https://arxiv.org/abs/1807.11164)

[\[1807.11626\] MnasNet: Platform-Aware Neural Architecture Search for Mobile](https://arxiv.org/abs/1807.11626)

[\[1808.00897\] BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation](https://arxiv.org/abs/1808.00897)

[\[1809.00219\] ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/1809.00219)

[\[1809.02983\] Dual Attention Network for Scene Segmentation](https://arxiv.org/abs/1809.02983)

[\[1811.08883\] Rethinking ImageNet Pre-training](https://arxiv.org/abs/1811.08883)

[\[1811.12231\] ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness](https://arxiv.org/abs/1811.12231)

[\[1812.01187\] Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187)

[\[1812.02391\] Meta-Transfer Learning for Few-Shot Learning](https://arxiv.org/abs/1812.02391)

[\[1812.03443\] FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search](https://arxiv.org/abs/1812.03443)

[\[1812.05050\] Fast Online Object Tracking and Segmentation: A Unifying Approach](https://arxiv.org/abs/1812.05050)

[\[1812.08928\] Slimmable Neural Networks](https://arxiv.org/abs/1812.08928)

[\[1812.11703\] SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks](https://arxiv.org/abs/1812.11703)

[\[1901.02446\] Panoptic Feature Pyramid Networks](https://arxiv.org/abs/1901.02446)

[\[1901.05103\] DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation](https://arxiv.org/abs/1901.05103)

[\[1902.09212\] Deep High-Resolution Representation Learning for Human Pose Estimation](https://arxiv.org/abs/1902.09212)

[\[1903.00241\] Mask Scoring R-CNN](https://arxiv.org/abs/1903.00241)

[\[1903.07291\] Semantic Image Synthesis with Spatially-Adaptive Normalization](https://arxiv.org/abs/1903.07291)

[\[1903.12355\] Local Aggregation for Unsupervised Learning of Visual Embeddings](https://arxiv.org/abs/1903.12355)

[\[1904.01355\] FCOS: Fully Convolutional One-Stage Object Detection](https://arxiv.org/abs/1904.01355)

[\[1904.05049\] Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution](https://arxiv.org/abs/1904.05049)

[\[1904.07392\] NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection](https://arxiv.org/abs/1904.07392)

[\[1904.07850\] Objects as Points](https://arxiv.org/abs/1904.07850)

[\[1905.02244\] Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)

[\[1905.08233\] Few-Shot Adversarial Learning of Realistic Neural Talking Head Models](https://arxiv.org/abs/1905.08233)

[\[1905.09272\] Data-Efficient Image Recognition with Contrastive Predictive Coding](https://arxiv.org/abs/1905.09272)

[\[1905.11946\] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)

[\[1906.05849\] Contrastive Multiview Coding](https://arxiv.org/abs/1906.05849)

[\[1906.06818\] Stacked Capsule Autoencoders](https://arxiv.org/abs/1906.06818)

[\[1907.02544\] Large Scale Adversarial Representation Learning](https://arxiv.org/abs/1907.02544)

[\[1907.05740\] Gated-SCNN: Gated Shape CNNs for Semantic Segmentation](https://arxiv.org/abs/1907.05740)

[\[1907.10786\] Interpreting the Latent Space of GANs for Semantic Face Editing](https://arxiv.org/abs/1907.10786)

[\[1907.11922\] MaskGAN: Towards Diverse and Interactive Facial Image Manipulation](https://arxiv.org/abs/1907.11922)

[\[1909.06161\] Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs](https://arxiv.org/abs/1909.06161)

[\[1911.04252\] Self-training with Noisy Student improves ImageNet classification](https://arxiv.org/abs/1911.04252)

[\[1911.05722\] Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722)

[\[1911.09070\] EfficientDet: Scalable and Efficient Object Detection](https://arxiv.org/abs/1911.09070)

[\[1912.01991\] Self-Supervised Learning of Pretext-Invariant Representations](https://arxiv.org/abs/1912.01991)

[\[1912.04958\] Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/abs/1912.04958)

[\[2001.00326\] NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search](https://arxiv.org/abs/2001.00326)

[\[2001.08735\] Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation](https://arxiv.org/abs/2001.08735)

[\[2002.05709\] A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)

[\[2003.04297\] Improved Baselines with Momentum Contrastive Learning](https://arxiv.org/abs/2003.04297)

[\[2003.04668\] Incremental Few-Shot Object Detection](https://arxiv.org/abs/2003.04668)

[\[2003.13678\] Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)

[\[2004.08955\] ResNeSt: Split-Attention Networks](https://arxiv.org/abs/2004.08955)

[\[2004.10934\] YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/abs/2004.10934)

[\[2005.05535\] DeepFaceLab: A simple, flexible and extensible face swapping framework](https://arxiv.org/abs/2005.05535)

[\[2005.10243\] What Makes for Good Views for Contrastive Learning?](https://arxiv.org/abs/2005.10243)

[\[2005.12872\] End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)

[\[2006.09882\] Unsupervised Learning of Visual Features by Contrasting Cluster Assignments](https://arxiv.org/abs/2006.09882)

[\[2010.11929\] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

[\[2011.10566\] Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566)

[\[2012.12877\] Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)

[\[2103.00020\] Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

[\[2103.03230\] Barlow Twins: Self-Supervised Learning via Redundancy Reduction](https://arxiv.org/abs/2103.03230)

[\[2103.14030\] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)

[\[2103.15459\] Capsule Network is Not More Robust than Convolutional Network](https://arxiv.org/abs/2103.15459)

[\[2104.07658\] Self-supervised Video Object Segmentation by Motion Grouping](https://arxiv.org/abs/2104.07658)

[\[2105.01601\] MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)

[\[2105.04906\] VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning](https://arxiv.org/abs/2105.04906)

[\[2107.08430\] YOLOX: Exceeding YOLO Series in 2021](https://arxiv.org/abs/2107.08430)

[\[2109.10852\] Pix2seq: A Language Modeling Framework for Object Detection](https://arxiv.org/abs/2109.10852)

[\[2110.07641\] Non-deep Networks](https://arxiv.org/abs/2110.07641)

[\[2111.06377\] Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)

### 自然语言处理

[\[1301.3781\] Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

[\[1303.5778\] Speech Recognition with Deep Recurrent Neural Networks](https://arxiv.org/abs/1303.5778)

[\[1308.0850\] Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850)

[\[1310.4546\] Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)

[\[1404.2188\] A Convolutional Neural Network for Modelling Sentences](https://arxiv.org/abs/1404.2188)

[\[1405.4053\] Distributed Representations of Sentences and Documents](https://arxiv.org/abs/1405.4053)

[\[1406.1078\] Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)

[\[1408.5882\] Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)

[\[1409.0473\] Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)

[\[1409.1259\] On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)

[\[1409.3215\] Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)

[\[1410.3916\] Memory Networks](https://arxiv.org/abs/1410.3916)

[\[1410.5401\] Neural Turing Machines](https://arxiv.org/abs/1410.5401)

[\[1412.5567\] Deep Speech: Scaling up end-to-end speech recognition](https://arxiv.org/abs/1412.5567)

[\[1503.00075\] Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](https://arxiv.org/abs/1503.00075)

[\[1505.00468\] VQA: Visual Question Answering](https://arxiv.org/abs/1505.00468)

[\[1506.03340\] Teaching Machines to Read and Comprehend](https://arxiv.org/abs/1506.03340)

[\[1506.05869\] A Neural Conversational Model](https://arxiv.org/abs/1506.05869)

[\[1506.07285\] Ask Me Anything: Dynamic Memory Networks for Natural Language Processing](https://arxiv.org/abs/1506.07285)

[\[1506.07503\] Attention-Based Models for Speech Recognition](https://arxiv.org/abs/1506.07503)

[\[1508.04025\] Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)

[\[1508.05326\] A large annotated corpus for learning natural language inference](https://arxiv.org/abs/1508.05326)

[\[1508.07909\] Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)

[\[1509.00685\] A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/abs/1509.00685)

[\[1509.01626\] Character-level Convolutional Networks for Text Classification](https://arxiv.org/abs/1509.01626)

[\[1510.03820\] A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1510.03820)

[\[1511.06709\] Improving Neural Machine Translation Models with Monolingual Data](https://arxiv.org/abs/1511.06709)

[\[1511.08308\] Named Entity Recognition with Bidirectional LSTM-CNNs](https://arxiv.org/abs/1511.08308)

[\[1512.02595\] Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/abs/1512.02595)

[\[1601.06733\] Long Short-Term Memory-Networks for Machine Reading](https://arxiv.org/abs/1601.06733)

[\[1602.02410\] Exploring the Limits of Language Modeling](https://arxiv.org/abs/1602.02410)

[\[1603.01354\] End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF](https://arxiv.org/abs/1603.01354)

[\[1603.01360\] Neural Architectures for Named Entity Recognition](https://arxiv.org/abs/1603.01360)

[\[1606.05250\] SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/abs/1606.05250)

[\[1607.01759\] Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759)

[\[1607.04606\] Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)

[\[1609.03499\] WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499)

[\[1609.08144\] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)

[\[1611.01603\] Bidirectional Attention Flow for Machine Comprehension](https://arxiv.org/abs/1611.01603)

[\[1612.08083\] Language Modeling with Gated Convolutional Networks](https://arxiv.org/abs/1612.08083)

[\[1701.02810\] OpenNMT: Open-Source Toolkit for Neural Machine Translation](https://arxiv.org/abs/1701.02810)

[\[1701.06547\] Adversarial Learning for Neural Dialogue Generation](https://arxiv.org/abs/1701.06547)

[\[1703.03130\] A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)

[\[1703.10135\] Tacotron: Towards End-to-End Speech Synthesis](https://arxiv.org/abs/1703.10135)

[\[1704.00051\] Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/abs/1704.00051)

[\[1704.04368\] Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368)

[\[1705.02364\] Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](https://arxiv.org/abs/1705.02364)

[\[1705.03122\] Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)

[\[1706.01427\] A simple neural network module for relational reasoning](https://arxiv.org/abs/1706.01427)

[\[1706.03762\] Attention Is All You Need](https://arxiv.org/abs/1706.03762)

[\[1707.07328\] Adversarial Examples for Evaluating Reading Comprehension Systems](https://arxiv.org/abs/1707.07328)

[\[1708.00107\] Learned in Translation: Contextualized Word Vectors](https://arxiv.org/abs/1708.00107)

[\[1712.05884\] Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/abs/1712.05884)

[\[1801.06146\] Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)

[\[1802.05365\] Deep contextualized word representations](https://arxiv.org/abs/1802.05365)

[\[1802.08435\] Efficient Neural Audio Synthesis](https://arxiv.org/abs/1802.08435)

[\[1806.03822\] Know What You Don't Know: Unanswerable Questions for SQuAD](https://arxiv.org/abs/1806.03822)

[\[1807.03819\] Universal Transformers](https://arxiv.org/abs/1807.03819)

[\[1808.06226\] SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/abs/1808.06226)

[\[1809.05679\] Graph Convolutional Networks for Text Classification](https://arxiv.org/abs/1809.05679)

[\[1809.07454\] Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation](https://arxiv.org/abs/1809.07454)

[\[1809.08267\] Neural Approaches to Conversational AI](https://arxiv.org/abs/1809.08267)

[\[1810.04805\] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

[\[1811.00002\] WaveGlow: A Flow-based Generative Network for Speech Synthesis](https://arxiv.org/abs/1811.00002)

[\[1901.02860\] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)

[\[1901.07291\] Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291)

[\[1901.08746\] BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/abs/1901.08746)

[\[1901.11117\] The Evolved Transformer](https://arxiv.org/abs/1901.11117)

[\[1901.11504\] Multi-Task Deep Neural Networks for Natural Language Understanding](https://arxiv.org/abs/1901.11504)

[\[1904.09223\] ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223)

[\[1904.09751\] The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)

[\[1904.10509\] Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)

[\[1905.02450\] MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/abs/1905.02450)

[\[1905.03197\] Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/abs/1905.03197)

[\[1905.07129\] ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/abs/1905.07129)

[\[1906.01502\] How multilingual is Multilingual BERT?](https://arxiv.org/abs/1906.01502)

[\[1906.04341\] What Does BERT Look At? An Analysis of BERT's Attention](https://arxiv.org/abs/1906.04341)

[\[1906.08237\] XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)

[\[1907.11692\] RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)

[\[1907.12412\] ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://arxiv.org/abs/1907.12412)

[\[1908.08530\] VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://arxiv.org/abs/1908.08530)

[\[1908.10084\] Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)

[\[1909.10351\] TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)

[\[1909.11942\] ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)

[\[1910.01108\] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)

[\[1910.10683\] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)

[\[1910.11856\] On the Cross-lingual Transferability of Monolingual Representations](https://arxiv.org/abs/1910.11856)

[\[1910.13461\] BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)

[\[1911.00536\] DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536)

[\[1911.02116\] Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)

[\[1911.03894\] CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)

[\[2001.04451\] Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)

[\[2003.10555\] ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)

[\[2004.05150\] Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)

[\[2005.14165\] Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

### 综述

[\[1206.5538\] Representation Learning: A Review and New Perspectives](https://arxiv.org/abs/1206.5538)

[\[1603.07285\] A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)

[\[1705.02801\] Graph Embedding Techniques, Applications, and Performance: A Survey](https://arxiv.org/abs/1705.02801)

[\[1709.07604\] A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications](https://arxiv.org/abs/1709.07604)

[\[1801.00553\] Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey](https://arxiv.org/abs/1801.00553)

[\[1802.00614\] Visual Interpretability for Deep Learning: a Survey](https://arxiv.org/abs/1802.00614)

[\[1802.03601\] Deep Visual Domain Adaptation: A Survey](https://arxiv.org/abs/1802.03601)

[\[1804.06655\] Deep Face Recognition: A Survey](https://arxiv.org/abs/1804.06655)

[\[1808.05377\] Neural Architecture Search: A Survey](https://arxiv.org/abs/1808.05377)

[\[1812.08434\] Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/abs/1812.08434)

[\[1901.00596\] A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/abs/1901.00596)

[\[1901.04407\] Self-Driving Cars: A Survey](https://arxiv.org/abs/1901.04407)

[\[1901.06032\] A Survey of the Recent Architectures of Deep Convolutional Neural Networks](https://arxiv.org/abs/1901.06032)

[\[1902.06068\] Deep Learning for Image Super-resolution: A Survey](https://arxiv.org/abs/1902.06068)

[\[1904.05046\] Generalizing from a Few Examples: A Survey on Few-Shot Learning](https://arxiv.org/abs/1904.05046)

[\[1904.08067\] Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067)

[\[1905.05055\] Object Detection in 20 Years: A Survey](https://arxiv.org/abs/1905.05055)

[\[1906.01529\] Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy](https://arxiv.org/abs/1906.01529)

[\[1907.09408\] A Survey of Deep Learning-based Object Detection](https://arxiv.org/abs/1907.09408)

[\[1907.12740\] Deep Learning in Video Multi-Object Tracking: A Survey](https://arxiv.org/abs/1907.12740)

[\[1912.00535\] Deep Learning for Visual Tracking: A Comprehensive Survey](https://arxiv.org/abs/1912.00535)

[\[2001.01582\] A Survey on Machine Reading Comprehension Systems](https://arxiv.org/abs/2001.01582)

[\[2001.05566\] Image Segmentation Using Deep Learning: A Survey](https://arxiv.org/abs/2001.05566)

[\[2002.00388\] A Survey on Knowledge Graphs: Representation, Acquisition and Applications](https://arxiv.org/abs/2002.00388)

[\[2003.02320\] Knowledge Graphs](https://arxiv.org/abs/2003.02320)

[\[2003.08271\] Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271)

[\[2004.05439\] Meta-Learning in Neural Networks: A Survey](https://arxiv.org/abs/2004.05439)

[\[2004.11149\] A Comprehensive Overview and Survey of Recent Advances in Meta-Learning](https://arxiv.org/abs/2004.11149)

[\[2006.01423\] Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods](https://arxiv.org/abs/2006.01423)

[\[2006.08218\] Self-supervised Learning: Generative or Contrastive](https://arxiv.org/abs/2006.08218)

[\[2006.14799\] Evaluation of Text Generation: A Survey](https://arxiv.org/abs/2006.14799)

[\[2009.06732\] Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)

[\[2102.10757\] Self-Supervised Learning of Graph Neural Networks: A Unified Review](https://arxiv.org/abs/2102.10757)

[\[2105.04387\] Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey](https://arxiv.org/abs/2105.04387)

[\[2109.06668\] Exploration in Deep Reinforcement Learning: A Comprehensive Survey](https://arxiv.org/abs/2109.06668)

[\[2111.07624\] Attention Mechanisms in Computer Vision: A Survey](https://arxiv.org/abs/2111.07624)

### 基准

[\[1604.06778\] Benchmarking Deep Reinforcement Learning for Continuous Control](https://arxiv.org/abs/1604.06778)

[\[1804.07461\] GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/abs/1804.07461)

[\[1810.00826\] How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826)

[\[1904.04232\] A Closer Look at Few-shot Classification](https://arxiv.org/abs/1904.04232)

[\[1905.00537\] SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems](https://arxiv.org/abs/1905.00537)

[\[1909.02729\] A Baseline for Few-Shot Image Classification](https://arxiv.org/abs/1909.02729)

[\[2003.00982\] Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982)

[\[2003.04390\] Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning](https://arxiv.org/abs/2003.04390)

[\[2003.11080\] XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization](https://arxiv.org/abs/2003.11080)

[\[2005.00687\] Open Graph Benchmark: Datasets for Machine Learning on Graphs](https://arxiv.org/abs/2005.00687)
